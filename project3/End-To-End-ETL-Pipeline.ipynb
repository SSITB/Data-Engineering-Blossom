{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us create our spark session\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"Stack Overflow Data Wrangling\")\n",
    "        .config(\"spark.jars\", \"../jars/postgresql-42.2.8.jar\")\n",
    "        .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1: Data Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's load all the datasets we'll be using.\n",
    "answers = spark.read.csv(\"stackoverflow/answers.csv\", header=True, inferSchema=True, multiLine=True)\n",
    "questions = spark.read.csv(\"stackoverflow/questions.csv\",header=True, inferSchema=True, multiLine=True)\n",
    "users = spark.read.csv(\"stackoverflow/users.csv\",header=True, inferSchema=True, multiLine=True)\n",
    "questiontags = spark.read.csv(\"stackoverflow/question_tags.csv\", header=True, inferSchema=True, multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'string'),\n",
       " ('user_id', 'string'),\n",
       " ('question_id', 'string'),\n",
       " ('body', 'string'),\n",
       " ('score', 'string'),\n",
       " ('comment_count', 'string'),\n",
       " ('created_at', 'string')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to know the shape of our dataframes\n",
    "def spark_shape(self):\n",
    "    return(self.count(), len(self.columns))\n",
    "pyspark.sql.dataframe.DataFrame.shape = spark_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9367215, 7)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's see the total number of rows and columns\n",
    "answers.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6773193, 9)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's see the shape of questions dataframe\n",
    "questions.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(273489, 12)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's see shape of users\n",
    "users.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(633700, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's also see the shape of question_tags\n",
    "questiontags.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'display_name',\n",
       " 'reputation',\n",
       " 'website_url',\n",
       " 'location',\n",
       " 'about_me',\n",
       " 'views',\n",
       " 'up_votes',\n",
       " 'down_votes',\n",
       " 'image_url',\n",
       " 'created_at',\n",
       " 'updated_at']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Overview of the columns in users dataframe\n",
    "users.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Records of Users = 273489\n",
      "+-------+------------+----------+--------------------+--------------------+--------+-----+--------+----------+--------------------+-------------------+-------------------+\n",
      "|     id|display_name|reputation|         website_url|            location|about_me|views|up_votes|down_votes|           image_url|         created_at|         updated_at|\n",
      "+-------+------------+----------+--------------------+--------------------+--------+-----+--------+----------+--------------------+-------------------+-------------------+\n",
      "|8357266|      suryan|         7|https://twitter.c...|Bangalore, Karnat...|    null|    8|       0|         0|https://www.grava...|2017-07-24 10:55:23|2019-06-19 05:00:16|\n",
      "|2602456|         Avi|         1|https://avtechtoo...|              Canada|    null|    0|       0|         0|                null|2013-07-20 15:10:25|2019-07-08 20:43:40|\n",
      "+-------+------------+----------+--------------------+--------------------+--------+-----+--------+----------+--------------------+-------------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Total Records of Users = {}'.format(users.count()))\n",
    "users.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            location|count|\n",
      "+--------------------+-----+\n",
      "|  Nowshera, Pakistan|    1|\n",
      "|           Bangalore|  165|\n",
      "|San Francisco Bay...|   18|\n",
      "|Eden Prairie, MN,...|    4|\n",
      "|     Beograd, Serbia|    4|\n",
      "|Cluj-Napoca, Cluj...|   33|\n",
      "|Montreal, Quebec,...|    2|\n",
      "|                Utah|   46|\n",
      "| Aalsmeer, Nederland|    1|\n",
      "|    Tlemcen, Algérie|    2|\n",
      "|Tirupur, Tamil Na...|    4|\n",
      "|São Gonçalo, RJ, ...|    1|\n",
      "|       Suzhou, China|    3|\n",
      "|Izmir, İzmir, Turkey|   11|\n",
      "| Bayern, Deutschland|   16|\n",
      "|       Toruń, Polska|    4|\n",
      "|Newtown, Kolkata,...|    1|\n",
      "|  Verona, VR, Italia|   19|\n",
      "|Santa Marta, Magd...|    1|\n",
      "|           kathmandu|    5|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Let's see the distinct countries we have.\n",
    "countries = users.groupBy('location').count()\n",
    "print(countries.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|      display_name|            location|\n",
      "+------------------+--------------------+\n",
      "|            suryan|Bangalore, Karnat...|\n",
      "|               Avi|              Canada|\n",
      "|              Matt|Pennsylvania, Uni...|\n",
      "|          Wing Fan|                null|\n",
      "|             A.Raw|New Delhi, Delhi,...|\n",
      "|           Ringo64|                null|\n",
      "|Hirotaka Nishimiya|          日本 Tōkyō|\n",
      "|           Anuroop|                null|\n",
      "|      Franco Buhay|                null|\n",
      "|     Kartik Juneja|Gharaunda, Haryan...|\n",
      "+------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users.select('display_name', 'location').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2: Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = users.withColumnRenamed('id', 'user_id').withColumnRenamed('created_at', 'user_created_at').withColumnRenamed('updated_at','user_updated_at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's store users coming from Canada in a new dataframe called country\n",
    "country = users.where(users.location.contains('Canada'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|        display_name|            location|\n",
      "+--------------------+--------------------+\n",
      "|                 Avi|              Canada|\n",
      "|               0-DAY|              Canada|\n",
      "|        Jeremy Banks|              Canada|\n",
      "|            siyi wei| Toronto, ON, Canada|\n",
      "|    Michael Sheinman| Grimsby, ON, Canada|\n",
      "|               James|British Columbia,...|\n",
      "|             Mohamed| Toronto, ON, Canada|\n",
      "|             PhillyJ|Newmarket, ON, Ca...|\n",
      "|               Simon|              Canada|\n",
      "|         Judd Foster|British Columbia,...|\n",
      "|                 MPG|Mississauga, ON, ...|\n",
      "|          Tejas Alva| Toronto, ON, Canada|\n",
      "|         e.b_al-issa|              Canada|\n",
      "|               Basil|Etobicoke, Toront...|\n",
      "|     MirageCommander|Montreal, QC, Canada|\n",
      "|       Alex O'Malley|              Canada|\n",
      "|           max pinch|Quebec City, QC, ...|\n",
      "|                Sare|              Canada|\n",
      "|                Doum|Québec City, QC, ...|\n",
      "|        Alex Manuele| Halifax, NS, Canada|\n",
      "|         user8507808|              Canada|\n",
      "|        Alex Boreham| Toronto, ON, Canada|\n",
      "|          Dayne Hack|North Vancouver, ...|\n",
      "|            Lewis921|Richmond, BC, Canada|\n",
      "|                Tanu|              Canada|\n",
      "|            Ryan Fwu|4617 Erwin Road, ...|\n",
      "|            sculpter| Toronto, ON, Canada|\n",
      "|        Jason Penner|Winnipeg, MB, Canada|\n",
      "|                 Pyr|Victoria, BC, Canada|\n",
      "|              hawker| Calgary, AB, Canada|\n",
      "|             deltron| Toronto, ON, Canada|\n",
      "|    needhelpwithlife|Kamloops, BC, Canada|\n",
      "|             infused|              Canada|\n",
      "|                Alik|Montreal, QC, Canada|\n",
      "|       Nathan Yaskiw| Calgary, AB, Canada|\n",
      "|    Steve R Laminger|Winnipeg, MB, Canada|\n",
      "|            Raboush2|London, Ontario, ...|\n",
      "|             Pagasur| Toronto, ON, Canada|\n",
      "|             Dominic|    Montreal, Canada|\n",
      "|           filipes95|Hamilton, ON, Canada|\n",
      "|              Vishal| Toronto, ON, Canada|\n",
      "|               Milos|Vancouver, BC, Ca...|\n",
      "|          Imran Juma|              Canada|\n",
      "|  Johnathon Douglass|              Canada|\n",
      "|             yoyo_24|Brampton, Ontario...|\n",
      "|             r3dm1ke| Toronto, ON, Canada|\n",
      "|        Tamon Suzuki|Montréal, QC, Canada|\n",
      "|               WR123| Toronto, ON, Canada|\n",
      "|         avengers123| Toronto, ON, Canada|\n",
      "|               simmy|              Canada|\n",
      "|            J.Griff2|  Ottawa, ON, Canada|\n",
      "|                 JGF|Vancouver, BC, Ca...|\n",
      "|            vasiliyx|Vancouver, BC, Ca...|\n",
      "|               Kenge|Montreal, QC, Canada|\n",
      "|          Shiqi Zhao|Waterloo, ON, Canada|\n",
      "|           DaGallane| Toronto, ON, Canada|\n",
      "|              Willow|              Canada|\n",
      "|                Marc|Montreal, QC, Canada|\n",
      "|           Im Yoonah|              Canada|\n",
      "|             Ding Ma|Montreal, QC, Canada|\n",
      "|       drag_me_under|              Canada|\n",
      "|      Nicolas Lortie|  Québec, QC, Canada|\n",
      "|              Lelik_|Montreal, QC, Canada|\n",
      "|          Songg Tùng| Burnaby, BC, Canada|\n",
      "|                 nms| Toronto, ON, Canada|\n",
      "|      Stephen Little| Nova Scotia, Canada|\n",
      "|               dan y|              Canada|\n",
      "|          iAMspecial|Abbotsford, BC, C...|\n",
      "|                 Ali|  Ottawa, ON, Canada|\n",
      "|              Pranoy|     Ontario, Canada|\n",
      "|             Patriot|              Canada|\n",
      "|          Shomzie324| Toronto, ON, Canada|\n",
      "|      mahyar khatiri|              Canada|\n",
      "|              Nerwin|              Canada|\n",
      "|               Abbat|Fort McMurray, AB...|\n",
      "|                 Jay|Edmonton, AB, Canada|\n",
      "|             Kannkor|              Canada|\n",
      "|          Kangil Lee| Toronto, ON, Canada|\n",
      "|               Kyuzo| Calgary, AB, Canada|\n",
      "|         Jason Jiang|              Canada|\n",
      "|     Achraf Ghellach|              Canada|\n",
      "|      Eliseos Mucaki|  London, ON, Canada|\n",
      "|             TruthIZ|  Québec, QC, Canada|\n",
      "|              Zheren|              Canada|\n",
      "|         Daniel Chen|              Canada|\n",
      "|                 tom| Toronto, ON, Canada|\n",
      "|          Adam Jones|  Ottawa, ON, Canada|\n",
      "|             nhtnhan|Edmonton, AB, Canada|\n",
      "|             Maria B|              Canada|\n",
      "|              JCrowe|  Ottawa, ON, Canada|\n",
      "|           Dimitro D| Toronto, ON, Canada|\n",
      "|                Ryan|North Vancouver, ...|\n",
      "|             JHeisey| Toronto, ON, Canada|\n",
      "|              xxyyxz|              Canada|\n",
      "|           Gaming4me|              Canada|\n",
      "|    Harkeerat Kanwal|Hamilton, ON, Canada|\n",
      "|             tiescut|  Ottawa, ON, Canada|\n",
      "|      Nader Samadyan|Vancouver, BC, Ca...|\n",
      "|          Harrylujah|              Canada|\n",
      "|lord and savior g...|              Canada|\n",
      "+--------------------+--------------------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Let us see the first 5 of our new dataframe\n",
    "country.select('display_name','location').show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3329, 12)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting city and contry into new columns\n",
    "city_coun = F.split(country['location'], ',')\n",
    "country = country.withColumn('city', city_coun.getItem(0))\n",
    "country = country.withColumn('country', city_coun.getItem(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+-------+\n",
      "|    display_name|   city|country|\n",
      "+----------------+-------+-------+\n",
      "|             Avi| Canada|   null|\n",
      "|           0-DAY| Canada|   null|\n",
      "|    Jeremy Banks| Canada|   null|\n",
      "|        siyi wei|Toronto| Canada|\n",
      "|Michael Sheinman|Grimsby| Canada|\n",
      "+----------------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Showing results after spliting city and country. \n",
    "#NB:We'll need to refine the code so as to make location with just country appear on country column.\n",
    "country.select('display_name','city','country').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming our columns which has same names as other columns from our datasets\n",
    "answers = answers.withColumnRenamed('id', 'answer_id')\n",
    "answers = answers.withColumnRenamed('created_at', 'answer_created_at')\n",
    "answers = answers.withColumnRenamed('body', 'answer_body')\n",
    "answers = answers.withColumnRenamed('score', 'answer_score')\n",
    "answers = answers.withColumnRenamed('comment_count', 'answer_comment_count')\n",
    "\n",
    "answers = answers.withColumnRenamed('answer_user_id', 'user_id')\n",
    "answers = answers.withColumnRenamed('ans_question_id', 'question_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['answer_id',\n",
       " 'user_id',\n",
       " 'question_id',\n",
       " 'answer_body',\n",
       " 'answer_score',\n",
       " 'answer_comment_count',\n",
       " 'answer_created_at']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming our column from id to user_id\n",
    "country = country.withColumnRenamed('id', 'user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user_id',\n",
       " 'display_name',\n",
       " 'reputation',\n",
       " 'website_url',\n",
       " 'location',\n",
       " 'about_me',\n",
       " 'views',\n",
       " 'up_votes',\n",
       " 'down_votes',\n",
       " 'image_url',\n",
       " 'user_created_at',\n",
       " 'user_updated_at',\n",
       " 'city',\n",
       " 'country']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's see our columns in our country dataframe after splitting location into city and country\n",
    "country.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['question_id',\n",
       " 'user_id',\n",
       " 'title',\n",
       " 'question_body',\n",
       " 'accepted_answer_id',\n",
       " 'question_score',\n",
       " 'view_count',\n",
       " 'comment_count',\n",
       " 'question_created_at']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's see the columns we have in our questions dataset\n",
    "questions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Noticing that some columns have names similar to the ones in users dataframe, we have to rename them.\n",
    "questions = questions.withColumnRenamed('id','question_id')\n",
    "questions = questions.withColumnRenamed('created_at', 'question_created_at')\n",
    "questions = questions.withColumnRenamed('body', 'question_body')\n",
    "questions = questions.withColumnRenamed('score', 'question_score')\n",
    "questions = questions.withColumnRenamed('question_comment_count', 'comment_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['question_id',\n",
       " 'user_id',\n",
       " 'title',\n",
       " 'question_body',\n",
       " 'accepted_answer_id',\n",
       " 'question_score',\n",
       " 'view_count',\n",
       " 'comment_count',\n",
       " 'question_created_at']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's confirm if our rename was successful\n",
    "questions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user_id',\n",
       " 'display_name',\n",
       " 'reputation',\n",
       " 'website_url',\n",
       " 'location',\n",
       " 'about_me',\n",
       " 'views',\n",
       " 'up_votes',\n",
       " 'down_votes',\n",
       " 'image_url',\n",
       " 'user_created_at',\n",
       " 'user_updated_at',\n",
       " 'city',\n",
       " 'country',\n",
       " 'question_id',\n",
       " 'title',\n",
       " 'question_body',\n",
       " 'accepted_answer_id',\n",
       " 'question_score',\n",
       " 'view_count',\n",
       " 'comment_count',\n",
       " 'question_created_at']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#An inner join of users in Canada and questions dataframe.\n",
    "users_country = country.join(questions, on='user_id', how='left')\n",
    "users_country.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's pick questions with at least 20 view counts.\n",
    "users_country = users_country.filter(users_country['view_count'] >= 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['question_id',\n",
       " 'user_id',\n",
       " 'display_name',\n",
       " 'reputation',\n",
       " 'website_url',\n",
       " 'location',\n",
       " 'about_me',\n",
       " 'views',\n",
       " 'up_votes',\n",
       " 'down_votes',\n",
       " 'image_url',\n",
       " 'user_created_at',\n",
       " 'user_updated_at',\n",
       " 'city',\n",
       " 'country',\n",
       " 'title',\n",
       " 'question_body',\n",
       " 'accepted_answer_id',\n",
       " 'question_score',\n",
       " 'view_count',\n",
       " 'comment_count',\n",
       " 'question_created_at',\n",
       " 'answer_id',\n",
       " 'answer_body',\n",
       " 'answer_score',\n",
       " 'answer_comment_count',\n",
       " 'answer_created_at',\n",
       " 'answer_id',\n",
       " 'answer_body',\n",
       " 'answer_score',\n",
       " 'answer_comment_count',\n",
       " 'answer_created_at',\n",
       " 'answer_id',\n",
       " 'answer_body',\n",
       " 'answer_score',\n",
       " 'answer_comment_count',\n",
       " 'answer_created_at']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Final task is to join our resultant table to the answers dataframe.\n",
    "users_country = users_country.join(answers, on=['question_id','user_id'], how='left')\n",
    "users_country.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3: Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1185.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 52.0 failed 1 times, most recent failure: Lost task 1.0 in stage 52.0 (TID 681, localhost, executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO stackoverflow_filtered.results (\"question_id\",\"user_id\",\"display_name\",\"reputation\",\"website_url\",\"location\",\"about_me\",\"views\",\"up_votes\",\"down_votes\",\"image_url\",\"user_created_at\",\"user_updated_at\",\"city\",\"country\",\"title\",\"question_body\",\"accepted_answer_id\",\"question_score\",\"view_count\",\"comment_count\",\"question_created_at\",\"answer_id\",\"answer_body\",\"answer_score\",\"answer_comment_count\",\"answer_created_at\",\"answer_id\",\"answer_body\",\"answer_score\",\"answer_comment_count\",\"answer_created_at\") VALUES ('54927792','10923030','t.lore','44',NULL,'London, ON, Canada',NULL,'17','11','0','https://www.gravatar.com/avatar/69b82512196f0b8773e826f7e8ac64e7?s=128&d=identicon&r=PG&f=1','2019-01-16 14:40:02','2019-05-17 18:31:42','London',' Canada','Error Code: 1054. Unknown column ''sdate'' in ''where clause''','<p>I got this error in my Query, do you have any idea how can I put the <code>sdate</code> in the 2 layer subquery?</p>\n\n<pre><code>select\nat.startDate as sdate, at.dau as DAU,\n(\nselect count(distinct d.uid) from\n (select ses.uid from dsession as ses where ses.startDate = sdate group by ses.uid\n  union all\n  select res.uid from rsession as res where res.startDate = sdate group by res.uid) as te\n) as MAU, (SELECT DAU/MAU) as AVG\nfrom\nattendance as at \n</code></pre>\n\n<p>it works if I query alone the subquery but when I merge it to the main query, the <code>sdate</code> got unknown. any idea?</p>\n\n<p>I tried to replace <code>sdate</code> on <code>where</code> as <code>at.startDate</code> but still got unknown <code>at.startDate</code> column.</p>\n','54928187','0','41','0','2019-02-28 14:20:09',NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL) was aborted: ERROR: column \"answer_id\" specified more than once\n  Position: 427  Call getNextException to see other errors in the batch.\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:148)\n\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:50)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2234)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:510)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:853)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1546)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:672)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:834)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:834)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.postgresql.util.PSQLException: ERROR: column \"answer_id\" specified more than once\n  Position: 427\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2497)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2233)\n\t... 18 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:935)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:933)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:933)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:834)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:68)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO stackoverflow_filtered.results (\"question_id\",\"user_id\",\"display_name\",\"reputation\",\"website_url\",\"location\",\"about_me\",\"views\",\"up_votes\",\"down_votes\",\"image_url\",\"user_created_at\",\"user_updated_at\",\"city\",\"country\",\"title\",\"question_body\",\"accepted_answer_id\",\"question_score\",\"view_count\",\"comment_count\",\"question_created_at\",\"answer_id\",\"answer_body\",\"answer_score\",\"answer_comment_count\",\"answer_created_at\",\"answer_id\",\"answer_body\",\"answer_score\",\"answer_comment_count\",\"answer_created_at\") VALUES ('54927792','10923030','t.lore','44',NULL,'London, ON, Canada',NULL,'17','11','0','https://www.gravatar.com/avatar/69b82512196f0b8773e826f7e8ac64e7?s=128&d=identicon&r=PG&f=1','2019-01-16 14:40:02','2019-05-17 18:31:42','London',' Canada','Error Code: 1054. Unknown column ''sdate'' in ''where clause''','<p>I got this error in my Query, do you have any idea how can I put the <code>sdate</code> in the 2 layer subquery?</p>\n\n<pre><code>select\nat.startDate as sdate, at.dau as DAU,\n(\nselect count(distinct d.uid) from\n (select ses.uid from dsession as ses where ses.startDate = sdate group by ses.uid\n  union all\n  select res.uid from rsession as res where res.startDate = sdate group by res.uid) as te\n) as MAU, (SELECT DAU/MAU) as AVG\nfrom\nattendance as at \n</code></pre>\n\n<p>it works if I query alone the subquery but when I merge it to the main query, the <code>sdate</code> got unknown. any idea?</p>\n\n<p>I tried to replace <code>sdate</code> on <code>where</code> as <code>at.startDate</code> but still got unknown <code>at.startDate</code> column.</p>\n','54928187','0','41','0','2019-02-28 14:20:09',NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL) was aborted: ERROR: column \"answer_id\" specified more than once\n  Position: 427  Call getNextException to see other errors in the batch.\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:148)\n\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:50)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2234)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:510)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:853)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1546)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:672)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:834)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:834)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.postgresql.util.PSQLException: ERROR: column \"answer_id\" specified more than once\n  Position: 427\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2497)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2233)\n\t... 18 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-c6c4a0abe4c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpassword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'postgres1234'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdbtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'stackoverflow_filtered.results'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m ).save(mode='append')\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    734\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1185.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 52.0 failed 1 times, most recent failure: Lost task 1.0 in stage 52.0 (TID 681, localhost, executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO stackoverflow_filtered.results (\"question_id\",\"user_id\",\"display_name\",\"reputation\",\"website_url\",\"location\",\"about_me\",\"views\",\"up_votes\",\"down_votes\",\"image_url\",\"user_created_at\",\"user_updated_at\",\"city\",\"country\",\"title\",\"question_body\",\"accepted_answer_id\",\"question_score\",\"view_count\",\"comment_count\",\"question_created_at\",\"answer_id\",\"answer_body\",\"answer_score\",\"answer_comment_count\",\"answer_created_at\",\"answer_id\",\"answer_body\",\"answer_score\",\"answer_comment_count\",\"answer_created_at\") VALUES ('54927792','10923030','t.lore','44',NULL,'London, ON, Canada',NULL,'17','11','0','https://www.gravatar.com/avatar/69b82512196f0b8773e826f7e8ac64e7?s=128&d=identicon&r=PG&f=1','2019-01-16 14:40:02','2019-05-17 18:31:42','London',' Canada','Error Code: 1054. Unknown column ''sdate'' in ''where clause''','<p>I got this error in my Query, do you have any idea how can I put the <code>sdate</code> in the 2 layer subquery?</p>\n\n<pre><code>select\nat.startDate as sdate, at.dau as DAU,\n(\nselect count(distinct d.uid) from\n (select ses.uid from dsession as ses where ses.startDate = sdate group by ses.uid\n  union all\n  select res.uid from rsession as res where res.startDate = sdate group by res.uid) as te\n) as MAU, (SELECT DAU/MAU) as AVG\nfrom\nattendance as at \n</code></pre>\n\n<p>it works if I query alone the subquery but when I merge it to the main query, the <code>sdate</code> got unknown. any idea?</p>\n\n<p>I tried to replace <code>sdate</code> on <code>where</code> as <code>at.startDate</code> but still got unknown <code>at.startDate</code> column.</p>\n','54928187','0','41','0','2019-02-28 14:20:09',NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL) was aborted: ERROR: column \"answer_id\" specified more than once\n  Position: 427  Call getNextException to see other errors in the batch.\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:148)\n\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:50)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2234)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:510)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:853)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1546)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:672)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:834)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:834)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.postgresql.util.PSQLException: ERROR: column \"answer_id\" specified more than once\n  Position: 427\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2497)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2233)\n\t... 18 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:935)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:933)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:933)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:834)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:68)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO stackoverflow_filtered.results (\"question_id\",\"user_id\",\"display_name\",\"reputation\",\"website_url\",\"location\",\"about_me\",\"views\",\"up_votes\",\"down_votes\",\"image_url\",\"user_created_at\",\"user_updated_at\",\"city\",\"country\",\"title\",\"question_body\",\"accepted_answer_id\",\"question_score\",\"view_count\",\"comment_count\",\"question_created_at\",\"answer_id\",\"answer_body\",\"answer_score\",\"answer_comment_count\",\"answer_created_at\",\"answer_id\",\"answer_body\",\"answer_score\",\"answer_comment_count\",\"answer_created_at\") VALUES ('54927792','10923030','t.lore','44',NULL,'London, ON, Canada',NULL,'17','11','0','https://www.gravatar.com/avatar/69b82512196f0b8773e826f7e8ac64e7?s=128&d=identicon&r=PG&f=1','2019-01-16 14:40:02','2019-05-17 18:31:42','London',' Canada','Error Code: 1054. Unknown column ''sdate'' in ''where clause''','<p>I got this error in my Query, do you have any idea how can I put the <code>sdate</code> in the 2 layer subquery?</p>\n\n<pre><code>select\nat.startDate as sdate, at.dau as DAU,\n(\nselect count(distinct d.uid) from\n (select ses.uid from dsession as ses where ses.startDate = sdate group by ses.uid\n  union all\n  select res.uid from rsession as res where res.startDate = sdate group by res.uid) as te\n) as MAU, (SELECT DAU/MAU) as AVG\nfrom\nattendance as at \n</code></pre>\n\n<p>it works if I query alone the subquery but when I merge it to the main query, the <code>sdate</code> got unknown. any idea?</p>\n\n<p>I tried to replace <code>sdate</code> on <code>where</code> as <code>at.startDate</code> but still got unknown <code>at.startDate</code> column.</p>\n','54928187','0','41','0','2019-02-28 14:20:09',NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL) was aborted: ERROR: column \"answer_id\" specified more than once\n  Position: 427  Call getNextException to see other errors in the batch.\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:148)\n\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:50)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2234)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:510)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:853)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1546)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:672)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:834)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:834)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.postgresql.util.PSQLException: ERROR: column \"answer_id\" specified more than once\n  Position: 427\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2497)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2233)\n\t... 18 more\n"
     ]
    }
   ],
   "source": [
    "#After creating our schema and table in sql. we now write our data from users_ques_ans dataframe into the table.\n",
    "#Let's use spark to write the results into this table\n",
    "users_country.write.format('jdbc').options(\n",
    "    url='jdbc:postgresql://localhost:5432/postgres',\n",
    "    driver ='org.postgresql.Driver',\n",
    "    user = 'postgres',\n",
    "    password = 'postgres1234',\n",
    "    dbtable = 'stackoverflow_filtered.results'\n",
    ").save(mode='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
