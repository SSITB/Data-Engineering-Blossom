{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us create our spark session\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using our boto3 client to download our files from s3 bucket\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3') # creating the boto3 to be able to access the file from the s3 bucket\n",
    "\n",
    "s3.download_file('blossom-data-engs', 'all-us-stocks-tickers-company-info-logos.zip', 'us_stocks.zip')\n",
    "s3.download_file('blossom-data-engs', 'data-scientist-job-market-in-the-us.zip', 'ds_jobs.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let us read our downloaded dataset with spark.\n",
    "companies = spark.read.csv(\"companies.csv\", header=True, inferSchema=True,multiLine=True)\n",
    "alldata = spark.read.csv(\"alldata.csv\", header=True, inferSchema=True, multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let rename 'description' column since it appears in both.\n",
    "companies = companies.withColumnRenamed('description','company_description')\n",
    "alldata = alldata.withColumnRenamed('description','job_description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time to merge the two data sets with an inner join.\n",
    "merged_data = companies.join(alldata, alldata['company'] == companies['company name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us now extract city from our location column\n",
    "merged_data = merged_data.select('*', F.split(alldata['location'], ',')[0].alias('city'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing NGRAM and Tokenizer to be used to generate ngrams.\n",
    "from pyspark.ml.feature import NGram, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our function to generate an ngram from a particular dataframe and column\n",
    "def create_ngram(df, col):\n",
    "    tokens = Tokenizer(inputCol=col, outputCol='tokens') # create tokens from the data on the col column\n",
    "    new_df = tokens.transform(df)   ## apply the tokenizer on the dataset\n",
    "    ngram = NGram(n=2, inputCol='tokens', outputCol='ngrams') # creating the ngram object\n",
    "    new_df = ngram.transform(new_df)  #transform the df with the ngram\n",
    "    return new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time to call our function.\n",
    "new_data = create_ngram(merged_data, 'company_description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------------------+\n",
      "|  location|  city|              ngrams|\n",
      "+----------+------+--------------------+\n",
      "|Austin, TX|Austin|[cubic corp, corp...|\n",
      "|Austin, TX|Austin|[the hershey, her...|\n",
      "+----------+------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_data.select(['location','city','ngrams']).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function to implement frequency count on a given column in a dataframe.\n",
    "def create_freq_df(df, col):\n",
    "    n=df.select(['ngrams',col]).select(col, F.explode('ngrams').alias('ngrams')).groupby([col, 'ngrams']).count() #exploding the ngrams\n",
    "    n = n.withColumnRenamed('count','frequency') # changing the column name from count to frequency\n",
    "    n = n.orderBy(n['frequency'].desc()) # ordering rows by biggest first.\n",
    "    return n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling our frequency function on the industry ngrams\n",
    "industry_freq_df = create_freq_df(new_data, 'industry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling our frequency function on the city ngrams\n",
    "city_freq_df = create_freq_df(new_data,'city')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------------+-----------------+\n",
      "|summary|            industry|           ngrams|        frequency|\n",
      "+-------+--------------------+-----------------+-----------------+\n",
      "|  count|                 702|              702|              702|\n",
      "|   mean|                null|             null|5.249287749287749|\n",
      "| stddev|                null|             null|9.044617147747154|\n",
      "|    min| Aerospace & Defense|       & casualty|                1|\n",
      "|    max|Retail - Apparel ...|www.ebay.com, its|               52|\n",
      "+-------+--------------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "industry_freq_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+---------+\n",
      "|     city|           ngrams|frequency|\n",
      "+---------+-----------------+---------+\n",
      "|Cambridge|       ability to|      130|\n",
      "|Cambridge|           in the|      130|\n",
      "|Cambridge|           of the|      104|\n",
      "|Cambridge|    experience in|       93|\n",
      "|Cambridge|           to the|       85|\n",
      "|San Diego|           in the|       82|\n",
      "|Cambridge|             in a|       79|\n",
      "|Cambridge|          and the|       78|\n",
      "|Cambridge|             as a|       67|\n",
      "|Cambridge|          to work|       65|\n",
      "|Cambridge|  experience with|       65|\n",
      "|Cambridge|equal opportunity|       64|\n",
      "|Cambridge|        sanofi is|       59|\n",
      "|Cambridge|          well as|       58|\n",
      "|Cambridge|          as well|       57|\n",
      "|Cambridge|     committed to|       56|\n",
      "|Cambridge|        regard to|       55|\n",
      "|Cambridge|          will be|       55|\n",
      "|San Diego|           of the|       54|\n",
      "|Cambridge|        pfizer is|       48|\n",
      "+---------+-----------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "city_freq_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
